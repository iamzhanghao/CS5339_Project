{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5339 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>airport '77 starts as a brand new luxury 747 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this film lacked something i couldn't put my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sorry everyone ,  ,  ,  i know this is suppose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\" it appears that many critics find the idea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the second attempt by a new york intellectual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i don't know who to blame ,  the timid writers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this film is mediocre at best .  angie harmon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the film is bad .  there is no other way to sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  story of a man who has unnatural feelings for ...\n",
       "1  airport '77 starts as a brand new luxury 747 p...\n",
       "2  this film lacked something i couldn't put my f...\n",
       "3  sorry everyone ,  ,  ,  i know this is suppose...\n",
       "4  when i was little my parents took me along to ...\n",
       "5   \" it appears that many critics find the idea ...\n",
       "6  the second attempt by a new york intellectual ...\n",
       "7  i don't know who to blame ,  the timid writers...\n",
       "8  this film is mediocre at best .  angie harmon ...\n",
       "9  the film is bad .  there is no other way to sa..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tempfile import mkstemp\n",
    "from shutil import move, copymode\n",
    "from os import fdopen, remove\n",
    "import re\n",
    "\n",
    "def replace(file_path):\n",
    "    #Create temp file\n",
    "    fh, abs_path = mkstemp()\n",
    "    with fdopen(fh,'w') as new_file:\n",
    "        with open(file_path) as old_file:\n",
    "            for line in old_file:\n",
    "                new_file.write(re.sub(\"  \" , \" \", line))\n",
    "\n",
    "    #Copy the file permissions from the old file to the new file\n",
    "    copymode(file_path, abs_path)\n",
    "    #Remove original file\n",
    "    remove(file_path)\n",
    "    #Move new file\n",
    "    move(abs_path, file_path)\n",
    "\n",
    "replace('aclImdb/test-neg.txt')\n",
    "replace('aclImdb/train-pos.txt')\n",
    "replace('aclImdb/test-pos.txt')\n",
    "replace('aclImdb/test-neg.txt')\n",
    "replace('aclImdb/train-unsup.txt')\n",
    "\n",
    "train_neg = pd.read_csv('aclImdb/train-neg.txt', header = None, delimiter = \"\\n\")\n",
    "train_pos = pd.read_csv('aclImdb/train-pos.txt', header = None, delimiter = \"\\n\")\n",
    "train_unsup = pd.read_csv('aclImdb/train-unsup.txt', header = None, delimiter = \"\\n\")\n",
    "test_pos = pd.read_csv('aclImdb/test-pos.txt', header = None, delimiter = \"\\n\")\n",
    "test_neg = pd.read_csv('aclImdb/test-neg.txt', header = None, delimiter = \"\\n\")\n",
    "\n",
    "\n",
    "all_train = pd.concat([train_neg, train_pos, train_unsup], ignore_index=True)\n",
    "\n",
    "all_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "with open('aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] \n",
    "        \n",
    "        # 25k train, 25k test, 50k unlabled data\n",
    "        split = ['train','test','extra','extra'][line_no//25000]  \n",
    "        # [12.5K pos, 12.5K neg]*2 then 50k unlabled data\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] \n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentDocument(words=['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'teachers', '\"', '.', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'teachers', '\"', '.', 'the', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'i', 'immediately', 'recalled', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'at', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'high', '.', 'a', 'classic', 'line', ':', 'inspector', ':', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'student', ':', 'welcome', 'to', 'bromwell', 'high', '.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', '.', 'what', 'a', 'pity', 'that', 'it', \"isn't\", '!'], tags=[0], split='train', sentiment=1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first document \n",
    "doc_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(cores)\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "# speed setup by sharing results of 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word vectors and doc vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2020-04-18 00:23:02.177846\n",
      "*0.497560 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 0.0s 0.4s\n",
      "*0.505600 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred 0.0s 5.6s\n",
      "*0.497560 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 0.0s 0.4s\n",
      "*0.505600 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred 0.0s 2.2s\n",
      "*0.497560 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 0.0s 0.4s\n",
      "*0.496800 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred 0.0s 2.8s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.410960 : 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 29.2s 0.4s\n",
      "*0.254600 : 2 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 12.2s 0.4s\n",
      "*0.261480 : 2 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 18.7s 0.5s\n",
      "completed pass 2 at alpha 0.023800\n",
      "*0.326200 : 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 49.2s 0.4s\n",
      "*0.127520 : 3 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 27.3s 0.8s\n",
      "*0.193800 : 3 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 39.7s 0.4s\n",
      "completed pass 3 at alpha 0.022600\n",
      "*0.237600 : 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 75.2s 0.4s\n",
      "*0.111160 : 4 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 36.2s 0.4s\n",
      "*0.171880 : 4 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 59.8s 0.4s\n",
      "completed pass 4 at alpha 0.021400\n",
      "*0.191880 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 103.2s 0.4s\n",
      "*0.206000 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred 103.2s 5.2s\n",
      "*0.106320 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 49.0s 0.5s\n",
      "*0.115600 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred 49.0s 2.9s\n",
      "*0.162040 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 76.6s 0.4s\n",
      "*0.201600 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred 76.6s 2.8s\n",
      "completed pass 5 at alpha 0.020200\n",
      "*0.171840 : 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 115.6s 0.4s\n",
      "*0.103880 : 6 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 66.9s 0.5s\n",
      "*0.153560 : 6 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 96.8s 0.4s\n",
      "completed pass 6 at alpha 0.019000\n",
      "*0.162200 : 7 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 133.2s 0.4s\n",
      " 0.106040 : 7 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 70.1s 0.7s\n",
      "*0.148880 : 7 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 110.1s 0.4s\n",
      "completed pass 7 at alpha 0.017800\n",
      "*0.158520 : 8 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 158.6s 0.4s\n",
      " 0.107040 : 8 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 81.5s 0.4s\n",
      "*0.143680 : 8 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 127.5s 0.4s\n",
      "completed pass 8 at alpha 0.016600\n",
      "*0.156240 : 9 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 171.2s 0.4s\n",
      " 0.106520 : 9 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 96.6s 0.4s\n",
      "*0.138920 : 9 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 143.9s 0.7s\n",
      "completed pass 9 at alpha 0.015400\n",
      "*0.153720 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 191.1s 0.4s\n",
      "*0.167200 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred 191.1s 4.9s\n",
      " 0.106160 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 104.6s 0.4s\n",
      "*0.112800 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred 104.6s 2.0s\n",
      "*0.137800 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 162.2s 0.4s\n",
      "*0.181200 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred 162.2s 2.8s\n",
      "completed pass 10 at alpha 0.014200\n",
      " 0.154440 : 11 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 210.4s 0.7s\n",
      " 0.107080 : 11 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 116.0s 0.4s\n",
      "*0.136000 : 11 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 179.8s 0.4s\n",
      "completed pass 11 at alpha 0.013000\n",
      " 0.154200 : 12 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 228.3s 0.4s\n",
      " 0.107520 : 12 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 128.2s 0.4s\n",
      "*0.133520 : 12 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 197.3s 0.4s\n",
      "completed pass 12 at alpha 0.011800\n",
      "*0.152560 : 13 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 255.7s 0.8s\n",
      " 0.108400 : 13 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 149.5s 0.4s\n",
      "*0.132440 : 13 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 215.4s 0.4s\n",
      "completed pass 13 at alpha 0.010600\n",
      " 0.153080 : 14 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 267.6s 0.4s\n",
      " 0.106120 : 14 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 153.7s 0.4s\n",
      "*0.131280 : 14 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 234.2s 0.4s\n",
      "completed pass 14 at alpha 0.009400\n",
      " 0.152960 : 15 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 286.2s 0.4s\n",
      " 0.172800 : 15 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred 286.2s 4.7s\n",
      " 0.106840 : 15 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 162.4s 0.7s\n",
      " 0.117200 : 15 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred 162.4s 2.0s\n",
      "*0.131280 : 15 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 251.5s 0.4s\n",
      " 0.194400 : 15 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred 251.5s 2.8s\n",
      "completed pass 15 at alpha 0.008200\n",
      " 0.152600 : 16 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 306.4s 0.4s\n",
      " 0.106800 : 16 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 174.4s 0.4s\n",
      "*0.131000 : 16 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 269.0s 0.7s\n",
      "completed pass 16 at alpha 0.007000\n",
      " 0.152920 : 17 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 333.6s 0.4s\n",
      " 0.107280 : 17 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 185.5s 0.4s\n",
      "*0.130560 : 17 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 287.4s 0.4s\n",
      "completed pass 17 at alpha 0.005800\n",
      "*0.151920 : 18 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 344.3s 0.4s\n",
      " 0.106320 : 18 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 202.7s 0.5s\n",
      " 0.130960 : 18 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 305.7s 0.4s\n",
      "completed pass 18 at alpha 0.004600\n",
      "*0.150320 : 19 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 363.2s 0.7s\n",
      " 0.107400 : 19 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 209.6s 0.4s\n",
      "*0.130280 : 19 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 323.9s 0.4s\n",
      "completed pass 19 at alpha 0.003400\n",
      " 0.150960 : 20 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 382.3s 0.4s\n",
      " 0.171600 : 20 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred 382.3s 4.8s\n",
      " 0.106680 : 20 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 222.2s 0.4s\n",
      " 0.113200 : 20 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred 222.2s 2.0s\n",
      " 0.130320 : 20 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 340.8s 0.7s\n",
      " 0.216800 : 20 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred 340.8s 2.8s\n",
      "completed pass 20 at alpha 0.002200\n",
      "END 2020-04-18 03:05:55.815503\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list,total_examples=100000, epochs=epoch)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
