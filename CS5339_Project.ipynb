{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5339 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>airport '77 starts as a brand new luxury 747 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this film lacked something i couldn't put my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sorry everyone ,  ,  ,  i know this is suppose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\" it appears that many critics find the idea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the second attempt by a new york intellectual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i don't know who to blame ,  the timid writers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this film is mediocre at best .  angie harmon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the film is bad .  there is no other way to sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  story of a man who has unnatural feelings for ...\n",
       "1  airport '77 starts as a brand new luxury 747 p...\n",
       "2  this film lacked something i couldn't put my f...\n",
       "3  sorry everyone ,  ,  ,  i know this is suppose...\n",
       "4  when i was little my parents took me along to ...\n",
       "5   \" it appears that many critics find the idea ...\n",
       "6  the second attempt by a new york intellectual ...\n",
       "7  i don't know who to blame ,  the timid writers...\n",
       "8  this film is mediocre at best .  angie harmon ...\n",
       "9  the film is bad .  there is no other way to sa..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tempfile import mkstemp\n",
    "from shutil import move, copymode\n",
    "from os import fdopen, remove\n",
    "import re\n",
    "\n",
    "def replace(file_path):\n",
    "    #Create temp file\n",
    "    fh, abs_path = mkstemp()\n",
    "    with fdopen(fh,'w') as new_file:\n",
    "        with open(file_path) as old_file:\n",
    "            for line in old_file:\n",
    "                new_file.write(re.sub(\"  \" , \" \", line))\n",
    "\n",
    "    #Copy the file permissions from the old file to the new file\n",
    "    copymode(file_path, abs_path)\n",
    "    #Remove original file\n",
    "    remove(file_path)\n",
    "    #Move new file\n",
    "    move(abs_path, file_path)\n",
    "\n",
    "replace('aclImdb/test-neg.txt')\n",
    "replace('aclImdb/train-pos.txt')\n",
    "replace('aclImdb/test-pos.txt')\n",
    "replace('aclImdb/test-neg.txt')\n",
    "replace('aclImdb/train-unsup.txt')\n",
    "\n",
    "train_neg = pd.read_csv('aclImdb/train-neg.txt', header = None, delimiter = \"\\n\")\n",
    "train_pos = pd.read_csv('aclImdb/train-pos.txt', header = None, delimiter = \"\\n\")\n",
    "train_unsup = pd.read_csv('aclImdb/train-unsup.txt', header = None, delimiter = \"\\n\")\n",
    "test_pos = pd.read_csv('aclImdb/test-pos.txt', header = None, delimiter = \"\\n\")\n",
    "test_neg = pd.read_csv('aclImdb/test-neg.txt', header = None, delimiter = \"\\n\")\n",
    "\n",
    "\n",
    "all_train = pd.concat([train_neg, train_pos, train_unsup], ignore_index=True)\n",
    "\n",
    "all_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "with open('aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] \n",
    "        \n",
    "        # 25k train, 25k test, 50k unlabled data\n",
    "        split = ['train','test','extra','extra'][line_no//25000]  \n",
    "        # [12.5K pos, 12.5K neg]*2 then 50k unlabled data\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] \n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentDocument(words=['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'teachers', '\"', '.', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'teachers', '\"', '.', 'the', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'i', 'immediately', 'recalled', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'at', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'high', '.', 'a', 'classic', 'line', ':', 'inspector', ':', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'student', ':', 'welcome', 'to', 'bromwell', 'high', '.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', '.', 'what', 'a', 'pity', 'that', 'it', \"isn't\", '!'], tags=[0], split='train', sentiment=1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first document \n",
    "doc_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(cores)\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "# speed setup by sharing results of 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=7, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word vectors and doc vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2020-04-19 13:55:43.414439\n",
      "*0.498200 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 0.0s 0.4s\n",
      "*0.483600 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred 0.0s 11.4s\n",
      "*0.498200 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 0.0s 0.4s\n",
      "*0.507200 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred 0.0s 4.6s\n",
      "*0.498200 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 0.0s 0.4s\n",
      "*0.502800 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred 0.0s 6.1s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.414760 : 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 26.0s 0.4s\n",
      "*0.253880 : 2 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 12.1s 0.4s\n",
      "*0.260120 : 2 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 18.1s 0.4s\n",
      "completed pass 2 at alpha 0.023800\n",
      "*0.317480 : 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 49.3s 0.4s\n",
      "*0.129840 : 3 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 23.6s 0.7s\n",
      "*0.192120 : 3 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 36.1s 0.4s\n",
      "completed pass 3 at alpha 0.022600\n",
      "*0.235440 : 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 72.2s 0.4s\n",
      "*0.112120 : 4 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 35.0s 0.5s\n",
      "*0.167360 : 4 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 54.6s 0.4s\n",
      "completed pass 4 at alpha 0.021400\n",
      "*0.193640 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 94.8s 0.4s\n",
      "*0.206000 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred 94.8s 10.9s\n",
      "*0.107760 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 46.4s 0.5s\n",
      "*0.118400 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred 46.4s 4.3s\n",
      "*0.158480 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 71.8s 0.4s\n",
      "*0.170400 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred 71.8s 5.6s\n",
      "completed pass 5 at alpha 0.020200\n",
      "*0.176720 : 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 117.2s 0.4s\n",
      "*0.106240 : 6 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 58.5s 0.4s\n",
      "*0.150280 : 6 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 89.8s 0.4s\n",
      "completed pass 6 at alpha 0.019000\n",
      "*0.163800 : 7 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 143.4s 0.4s\n",
      "*0.105720 : 7 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 70.6s 0.7s\n",
      "*0.146080 : 7 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 108.0s 0.4s\n",
      "completed pass 7 at alpha 0.017800\n",
      "*0.159040 : 8 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 161.4s 0.4s\n",
      " 0.107000 : 8 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 86.2s 0.4s\n",
      "*0.142320 : 8 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 126.0s 0.4s\n",
      "completed pass 8 at alpha 0.016600\n",
      "*0.157080 : 9 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 189.1s 0.5s\n",
      " 0.107000 : 9 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 113.3s 0.6s\n",
      "*0.140320 : 9 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 151.3s 0.7s\n",
      "completed pass 9 at alpha 0.015400\n",
      "*0.155960 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 211.3s 0.4s\n",
      "*0.165600 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred 211.3s 10.9s\n",
      " 0.107680 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 106.6s 0.4s\n",
      "*0.104000 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred 106.6s 4.1s\n",
      "*0.137440 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 169.9s 0.4s\n",
      "*0.145600 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred 169.9s 6.3s\n",
      "completed pass 10 at alpha 0.014200\n",
      "*0.155040 : 11 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 225.1s 0.7s\n",
      " 0.108080 : 11 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 117.6s 0.5s\n",
      "*0.135760 : 11 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 195.8s 0.4s\n",
      "completed pass 11 at alpha 0.013000\n",
      " 0.155560 : 12 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 247.4s 0.5s\n",
      " 0.107840 : 12 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 139.8s 0.5s\n",
      "*0.133240 : 12 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 205.8s 0.5s\n",
      "completed pass 12 at alpha 0.011800\n",
      "*0.154880 : 13 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 257.3s 0.8s\n",
      " 0.109040 : 13 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 146.6s 0.4s\n",
      "*0.132840 : 13 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 224.7s 0.4s\n",
      "completed pass 13 at alpha 0.010600\n",
      "*0.154440 : 14 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 290.5s 0.4s\n",
      " 0.109960 : 14 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 151.1s 0.4s\n",
      "*0.131800 : 14 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 232.7s 0.4s\n",
      "completed pass 14 at alpha 0.009400\n",
      " 0.154760 : 15 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 290.8s 0.4s\n",
      "*0.163600 : 15 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred 290.8s 10.4s\n",
      " 0.109200 : 15 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 174.3s 0.7s\n",
      " 0.118000 : 15 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred 174.3s 3.9s\n",
      "*0.131600 : 15 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 254.6s 0.4s\n",
      " 0.146400 : 15 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred 254.6s 5.6s\n",
      "completed pass 15 at alpha 0.008200\n",
      "*0.154080 : 16 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 317.5s 0.4s\n",
      " 0.109640 : 16 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 185.8s 0.4s\n",
      "*0.130720 : 16 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 285.4s 0.7s\n",
      "completed pass 16 at alpha 0.007000\n",
      " 0.156560 : 17 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 358.5s 0.4s\n",
      " 0.108160 : 17 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 194.5s 0.4s\n",
      "*0.129720 : 17 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 296.9s 0.4s\n",
      "completed pass 17 at alpha 0.005800\n",
      " 0.155400 : 18 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 392.4s 0.4s\n",
      " 0.108400 : 18 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 211.6s 0.5s\n",
      " 0.130000 : 18 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 328.0s 0.4s\n",
      "completed pass 18 at alpha 0.004600\n",
      " 0.154440 : 19 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 383.1s 0.7s\n",
      " 0.109400 : 19 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 211.8s 0.5s\n",
      "*0.128960 : 19 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 337.0s 0.4s\n",
      "completed pass 19 at alpha 0.003400\n",
      " 0.155480 : 20 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12) 411.3s 0.5s\n",
      " 0.166000 : 20 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred 411.3s 10.7s\n",
      " 0.109400 : 20 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12) 235.1s 0.5s\n",
      "*0.102800 : 20 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred 235.1s 4.2s\n",
      " 0.129240 : 20 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12) 355.2s 0.7s\n",
      " 0.167600 : 20 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred 355.2s 5.7s\n",
      "completed pass 20 at alpha 0.002200\n",
      "END 2020-04-19 16:46:19.949870\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list,total_examples=100000, epochs=epoch)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.102800 Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)_inferred\n",
      "0.105720 Doc2Vec(dbow,d100,n5,mc2,s0.001,t12)\n",
      "0.128960 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)\n",
      "0.145600 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t12)_inferred\n",
      "0.154080 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)\n",
      "0.163600 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t12)_inferred\n"
     ]
    }
   ],
   "source": [
    "# print best error rates achieved\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
