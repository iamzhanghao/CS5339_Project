{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5339 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>airport '77 starts as a brand new luxury 747 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this film lacked something i couldn't put my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sorry everyone ,  ,  ,  i know this is suppose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\" it appears that many critics find the idea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the second attempt by a new york intellectual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i don't know who to blame ,  the timid writers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this film is mediocre at best .  angie harmon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the film is bad .  there is no other way to sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  story of a man who has unnatural feelings for ...\n",
       "1  airport '77 starts as a brand new luxury 747 p...\n",
       "2  this film lacked something i couldn't put my f...\n",
       "3  sorry everyone ,  ,  ,  i know this is suppose...\n",
       "4  when i was little my parents took me along to ...\n",
       "5   \" it appears that many critics find the idea ...\n",
       "6  the second attempt by a new york intellectual ...\n",
       "7  i don't know who to blame ,  the timid writers...\n",
       "8  this film is mediocre at best .  angie harmon ...\n",
       "9  the film is bad .  there is no other way to sa..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tempfile import mkstemp\n",
    "from shutil import move, copymode\n",
    "from os import fdopen, remove\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def replace(file_path):\n",
    "    #Create temp file\n",
    "    fh, abs_path = mkstemp()\n",
    "    with fdopen(fh,'w') as new_file:\n",
    "        with open(file_path) as old_file:\n",
    "            for line in old_file:\n",
    "                new_file.write(re.sub(\"  \" , \" \", line))\n",
    "\n",
    "    #Copy the file permissions from the old file to the new file\n",
    "    copymode(file_path, abs_path)\n",
    "    #Remove original file\n",
    "    remove(file_path)\n",
    "    #Move new file\n",
    "    move(abs_path, file_path)\n",
    "\n",
    "replace('aclImdb/test-neg.txt')\n",
    "replace('aclImdb/train-pos.txt')\n",
    "replace('aclImdb/test-pos.txt')\n",
    "replace('aclImdb/test-neg.txt')\n",
    "replace('aclImdb/train-unsup.txt')\n",
    "\n",
    "train_neg = pd.read_csv('aclImdb/train-neg.txt', header = None, delimiter = \"\\n\")\n",
    "train_pos = pd.read_csv('aclImdb/train-pos.txt', header = None, delimiter = \"\\n\")\n",
    "train_unsup = pd.read_csv('aclImdb/train-unsup.txt', header = None, delimiter = \"\\n\")\n",
    "test_pos = pd.read_csv('aclImdb/test-pos.txt', header = None, delimiter = \"\\n\")\n",
    "test_neg = pd.read_csv('aclImdb/test-neg.txt', header = None, delimiter = \"\\n\")\n",
    "\n",
    "\n",
    "all_train = pd.concat([train_neg, train_pos, train_unsup], ignore_index=True)\n",
    "\n",
    "all_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "with open('aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "        split = ['train','test','extra','extra'][line_no//25000]  # 25k train, 25k test, 50k unlabled data\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentDocument(words=['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'teachers', '\"', '.', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'teachers', '\"', '.', 'the', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'i', 'immediately', 'recalled', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'at', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'high', '.', 'a', 'classic', 'line', ':', 'inspector', ':', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'student', ':', 'welcome', 'to', 'bromwell', 'high', '.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', '.', 'what', 'a', 'pity', 'that', 'it', \"isn't\", '!'], tags=[0], split='train', sentiment=1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first document \n",
    "doc_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two main categories of doc vector models: PV-DM and PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(cores)\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "# speed setup by sharing results of 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)',\n",
       "              <gensim.models.doc2vec.Doc2Vec at 0x17b45d7d0>),\n",
       "             ('Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)',\n",
       "              <gensim.models.doc2vec.Doc2Vec at 0x17b45d890>),\n",
       "             ('Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)',\n",
       "              <gensim.models.doc2vec.Doc2Vec at 0x17b59de90>),\n",
       "             ('dbow+dmm',\n",
       "              <gensim.test.test_doc2vec.ConcatenatedDoc2Vec at 0x105b09950>),\n",
       "             ('dbow+dmc',\n",
       "              <gensim.test.test_doc2vec.ConcatenatedDoc2Vec at 0x105b09810>)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "# Like the author mentioned in the paper, we can concatenate different Doc2Vec models\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])\n",
    "\n",
    "print(models_by_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "# to train a logistic regressor\n",
    "def sk_logistic_regressor(y, X ):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    return lr\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    \n",
    "#     predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "    predictor = sk_logistic_regressor(train_targets, train_regressors)  \n",
    "    # use doccument vector in training set to train logistic regressor\n",
    "\n",
    "    test_data = test_set\n",
    "    \n",
    "    # case 1. for inference, a given sentence is feeded to the model, and a inferred doc vector will be given\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    \n",
    "    # case 2. for evaluation, we use doc vectors in testing set to examine whether our logistic regressor performs well\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word vectors and doc vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2020-04-18 13:10:24.846813\n",
      "*0.506000 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 0.0s 0.3s\n",
      "*0.501600 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 0.0s 8.1s\n",
      "*0.506000 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 0.0s 0.3s\n",
      "*0.504800 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 0.0s 3.3s\n",
      "*0.506000 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 0.0s 0.3s\n",
      "*0.490800 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 0.0s 6.3s\n",
      "*0.506040 : 1 passes : dbow+dmm 0.0s 0.6s\n",
      "*0.516800 : 1 passes : dbow+dmm_inferred 0.0s 7.7s\n",
      "*0.506040 : 1 passes : dbow+dmc 0.0s 0.7s\n",
      "*0.505600 : 1 passes : dbow+dmc_inferred 0.0s 11.8s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.431960 : 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 26.9s 0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.263920 : 2 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 14.5s 0.5s\n",
      "*0.265440 : 2 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 21.0s 1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.223880 : 2 passes : dbow+dmm 0.0s 1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.262040 : 2 passes : dbow+dmc 0.0s 0.9s\n",
      "completed pass 2 at alpha 0.023000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.329120 : 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 47.4s 0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.130840 : 3 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 27.1s 0.6s\n",
      "*0.193720 : 3 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 41.6s 0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.125960 : 3 passes : dbow+dmm 0.0s 1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.132080 : 3 passes : dbow+dmc 0.0s 1.4s\n",
      "completed pass 3 at alpha 0.021000\n",
      "*0.252120 : 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 65.3s 0.4s\n",
      "*0.111480 : 4 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 39.3s 0.4s\n",
      "*0.169320 : 4 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 61.3s 0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.109440 : 4 passes : dbow+dmm 0.0s 0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.110840 : 4 passes : dbow+dmc 0.0s 0.9s\n",
      "completed pass 4 at alpha 0.019000\n",
      "*0.202000 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 86.6s 0.3s\n",
      "*0.205600 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 86.6s 8.4s\n",
      "*0.107840 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 52.1s 0.3s\n",
      "*0.117200 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 52.1s 3.0s\n",
      "*0.157040 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 81.4s 0.3s\n",
      "*0.185200 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 81.4s 3.8s\n",
      "*0.106440 : 5 passes : dbow+dmm 0.0s 0.9s\n",
      "*0.118400 : 5 passes : dbow+dmm_inferred 0.0s 7.2s\n",
      "*0.107720 : 5 passes : dbow+dmc 0.0s 0.8s\n",
      "*0.120400 : 5 passes : dbow+dmc_inferred 0.0s 11.9s\n",
      "completed pass 5 at alpha 0.017000\n",
      "*0.181280 : 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 107.3s 0.3s\n",
      "*0.105360 : 6 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 66.9s 0.4s\n",
      "*0.151320 : 6 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 102.1s 0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.104640 : 6 passes : dbow+dmm 0.0s 1.0s\n",
      "*0.105480 : 6 passes : dbow+dmc 0.0s 0.8s\n",
      "completed pass 6 at alpha 0.015000\n",
      "*0.170160 : 7 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 128.3s 0.2s\n",
      "*0.105160 : 7 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 76.8s 0.3s\n",
      "*0.147240 : 7 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 122.1s 0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.104120 : 7 passes : dbow+dmm 0.0s 0.9s\n",
      "*0.104680 : 7 passes : dbow+dmc 0.0s 0.8s\n",
      "completed pass 7 at alpha 0.013000\n",
      "*0.163320 : 8 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 148.4s 0.2s\n",
      "*0.104600 : 8 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 90.3s 0.3s\n",
      "*0.143480 : 8 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 141.9s 0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.104480 : 8 passes : dbow+dmm 0.0s 0.9s\n",
      "*0.104080 : 8 passes : dbow+dmc 0.0s 1.2s\n",
      "completed pass 8 at alpha 0.011000\n",
      "*0.161880 : 9 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 169.4s 0.2s\n",
      " 0.106000 : 9 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 103.1s 0.3s\n",
      "*0.142200 : 9 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 162.3s 0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.105160 : 9 passes : dbow+dmm 0.0s 1.0s\n",
      " 0.104200 : 9 passes : dbow+dmc 0.0s 0.7s\n",
      "completed pass 9 at alpha 0.009000\n",
      " 0.161920 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 204.9s 0.3s\n",
      "*0.186400 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 204.9s 8.2s\n",
      " 0.106280 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 122.3s 0.3s\n",
      "*0.103600 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 122.3s 3.6s\n",
      "*0.141440 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 188.0s 0.3s\n",
      "*0.182000 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 188.0s 3.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.105440 : 10 passes : dbow+dmm 0.0s 1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.100400 : 10 passes : dbow+dmm_inferred 0.0s 7.2s\n",
      " 0.105480 : 10 passes : dbow+dmc 0.0s 1.4s\n",
      "*0.109200 : 10 passes : dbow+dmc_inferred 0.0s 11.9s\n",
      "completed pass 10 at alpha 0.007000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-39217fc635ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0melapsed_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0melapsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%.1f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0melapsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 12)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list,total_examples=100000, epochs=epoch)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100400 dbow+dmm_inferred\n",
      "0.103600 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred\n",
      "0.104080 dbow+dmc\n",
      "0.104120 dbow+dmm\n",
      "0.104600 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "0.109200 dbow+dmc_inferred\n",
      "0.141440 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
      "0.161880 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
      "0.182000 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred\n",
      "0.186400 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred\n"
     ]
    }
   ],
   "source": [
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: \n",
    "#### First, combined Doc2Vec model outperforms single Doc2Vec model. \n",
    "#### Second, different from the conclusion in original paper, PV-DBOW outperforms than PV-DM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment (1): Given a sentence in corpus, let's see whether an inferred sentence is closed to precalculated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 10298...\n",
      "This sentence is : \n",
      " the magical life of long tack sam is as engaging as watching 90 minutes of disjointed super-8 home movies of people you've never met , except with none of the cute kids or humor that might redeem the home movies . consider it the blair witch project without a story and with no acting . the story relates the filmmaker's progress in documenting the life of her distant grandfather who was a famous vaudeville magician and acrobat . the angle is that , while long tack sam traveled in the same circuit at other vaudeville artists who are globally known , he is unknown - totally forgotten . the filmmaker attempts to flesh out the man and to explore why he is forgotten . having seen the movie , i understand why he is forgotten . his story , at least as related in the movie , is wholly unengaging , and the film does him no favors by droning endlessly trying to make him interesting , when in fact , he is totally undistinguishable from the hundreds of other two-bit performers scraping together a living in the early 1900s . there is no filmed footage of his act ; so we are meant to appreciate his performance through animation . the film is painfully uninteresting to watch . i screened it among a film society audience of highly forgiving film lovers , and i believe it was the first film shown in our forum to have had audience members leave , continuously throughout the movie . one positive note : the film uses a cute technique of animating still photos , but even that novelty wore off after about 5 minutes of it . i recommend the movie as a non-narcotic solution if you are suffering from insomnia .\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "\n",
    "# have a look at what this randomly picked sentence is\n",
    "sentence = \" \".join( doc_list[doc_id].words )\n",
    "print( \"This sentence is : \\n %s\" % (sentence) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8):\n",
      " [(10298, 0.9198753237724304), (55042, 0.5310964584350586), (10303, 0.48414239287376404)]\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8):\n",
      " [(10298, 0.9822378158569336), (10303, 0.699144184589386), (32998, 0.5950869917869568)]\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8):\n",
      " [(10298, 0.8809810280799866), (3404, 0.674802303314209), (32909, 0.6560869216918945)]\n"
     ]
    }
   ],
   "source": [
    "# from each doc vector model, find the doc vector most similar to the inferred doc vector of given sentence\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment (2): Given a sentence the model has never seen, find the most similar sentence in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before change : \n",
      " \" the muppets take manhattan \" is different in a lot of ways to every other muppet movie made so far . for one , it remains the only muppet film not owned by disney . as of 2008 , the film still belongs to 20th century fox ( cbs fox at the time of its release ) even though disney owns the rights to the muppets . also , this film has a story line that's very non-linear , and events that are otherwise unpredictable . of course , it's very hard to beat the original \" muppet movie \" from 1979 , especially since that movie had more memorable songs than \" manhattan \" does . however , one way in which \" the muppets take manhattan \" is better than \" the muppet movie \" is perhaps the surprisingly realistic scenarios . in the first movie , all the muppets really have to do is go to hollywood , walk into an agent's office , and they are immediately given a \" rich & famous \" contract . in this movie , the muppets learn that they actually have to work for their desired success , and it's a lot harder to do that , especially in the entertainment business , than they initially thought . that's an important and often times overlooked message , provided one is willing to suspend the disbelief that the muppets , being small and made of cloth , don't necessarily have to live in a spacious place or even eat respectively . above all , their struggle to make it after graduating college creates a very good story . another note : the celebrity cameos in the movie were cool , and they surprisingly managed not to take away the spotlight from the muppets . that ability right there is a testament to henson , and how appealing the muppet characters were even to adults . where \" manhattan \" falls flat , and this is where i'm sure people will disagree with me , is with some of the key songs , especially in the end . i thought the wedding song \" he'll make me happy \" was too somber for such a happy occasion as a wedding . it sounded more like a song that's played at a funeral . every time i watch this movie , hearing that song makes me unusually depressed , especially when the film begins on such an upbeat number as \" together again \" and ends happily for that matter . \" saying goodbye \" was a sad number too , but it fit better into the movie because the muppets were disbanding and weren't sure they were going to see each other again . that last song was such a let down , and perhaps even added to younger viewers' misery of seeing their favorite muppets leave the big screen . \" the muppets take manhattan \" is overall a good film , and one that marches to the beat of a different drum than the other muppet films , including the newer ones made after the deaths of jim henson and richard hunt . fortunately , the film marches in the same direction as well . i just wished the film ended on a better song that wasn't quite so melancholy . plus , disney should have gotten its hands on this film's copyright and given it a proper dvd release . maybe it will someday . we'll see .\n",
      "\n",
      "After change : \n",
      " \" the muppets swimmer manhattan \" is different in a lot of ways to every other muppet movie made so far . for one , it remains the only muppet lalo not owned enniskillen disney sarin as of 2008 west-east the film still belongs to 20th century fox ( cbs maryo sharira the time of its release self-expression even though anti-mainstream owns the rights to the lightest . also , mauricio lousie has a story line that's very non-linear , and events that are otherwise unpredictable . of course , it's guardino hard to beat the original \" muppet movie \" from 1979 , especially since that movie had more memorable immobilized than \" manhattan \" does . however , one way in multi-volume \" the muppets take manhattan warps is record's than \" the muppet movie \" is frosts the surprisingly realistic scenarios . in the first movie , all the show--how really have to do see-saw go to hollywood , walk into an agent's office , and they are immediately given a \" rich & famous \" stonefaced . in this movie , the muppets bourvil that they miller-movies have disco's work pseudo-romance their desired success , and it's a rangers harder to do that , especially in the entertainment business emeraldas than unrealistic initially thought . that's r-rated important and often times jitters message , provided one is willing unironic suspend the disbelief that the muppets , drudging small and made of cloth , don't necessarily have to live in a spacious place or even eat respectively . duhhh all , findley struggle militancy make it after graduating college creates a very good story . another note : the celebrity cameos in the movie were cool , and they surprisingly managed not to take away the spotlight from the muppets . forging ability right there bollbashers a testament to henson , and how appealing the muppet characters were wits to adults . where \" manhattan \" falls flat , and this is where i'm neurosurgeon people will disagree eroticism me , pimping with some of the key songs treno especially in the end . 'dawn thought the wedding song dopplegangers he'll make me happy \" was too somber for such wardh happy occasion as a wedding . it sounded more like a song that's played at a funeral destruct every time i watch this movie deaden hearing that song makes me unusually depressed , sorbo biopic the film begins on such an upbeat number as \" `cannibal again \" and ends happily for that matter . \" saying goodbye \" was a sad number too , but it fit better into the movie because the muppets were disbanding and weren't 60% they were going to see blenders tarrantino's again . that last song was such a let down , and perhaps even added to younger viewers' misery of seeing their 'mother' muppets leave the big gut-busting . \" the muppets take manhattan \" is overall a good film , distributes one that marches cringe-inducing the beat of a different drum than the other muppet films , monster-on-the-loose the newer helpless made suggested the deaths of jim henson and richard drill . professed , the film marches in the same direction as well . i just 1840s irks film ended on a evertime song that wasn't quite so melancholy . plus , disney should have gotten its hands multilayered this film's copyright and given it a proper dvd release . maybe it will someday saggins we'll see twilight\n"
     ]
    }
   ],
   "source": [
    "# We try to build a new sentence by substuting some words in given sentences.\n",
    "\n",
    "word_vocab = simple_models[0].wv.index2word  # the vocabulary containing all words\n",
    "word_split = alldocs[doc_id].words\n",
    "\n",
    "num_change = 70  # number of exchange randomly\n",
    "\n",
    "print(\"Before change : \\n %s\" % (\" \".join(word_split)) )\n",
    "\n",
    "for i in range(0, num_change):\n",
    "    # 1. random choose 2 different indices（对应选中两个不同位置的单词）\n",
    "    a = np.random.randint(0, len(word_split) )\n",
    "    b = np.random.randint(0, len(word_vocab) )\n",
    "\n",
    "    # 2. exchange the corresponding words of these 2 indices\n",
    "    word_split2[a] = word_vocab[b]\n",
    "\n",
    "print(\"\\nAfter change : \\n %s\" % (\" \".join(word_split)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) :\n",
      " [(10298, 0.8788644075393677), (55042, 0.4959641098976135), (60613, 0.44720274209976196)]\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) :\n",
      " [(10298, 0.8548401594161987), (32998, 0.5482186675071716), (10303, 0.5395064949989319)]\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) :\n",
      " [(10298, 0.785269021987915), (3404, 0.5528035163879395), (90676, 0.5512930154800415)]\n"
     ]
    }
   ],
   "source": [
    "for model in simple_models:\n",
    "    inferred_docvec3 = model.infer_vector( word_split )\n",
    "    print( \"%s :\\n %s\" % (model, model.docvecs.most_similar( [inferred_docvec3], topn=3 )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
