{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5339 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>airport '77 starts as a brand new luxury 747 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this film lacked something i couldn't put my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sorry everyone ,  ,  ,  i know this is suppose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\" it appears that many critics find the idea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the second attempt by a new york intellectual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i don't know who to blame ,  the timid writers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this film is mediocre at best .  angie harmon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the film is bad .  there is no other way to sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  story of a man who has unnatural feelings for ...\n",
       "1  airport '77 starts as a brand new luxury 747 p...\n",
       "2  this film lacked something i couldn't put my f...\n",
       "3  sorry everyone ,  ,  ,  i know this is suppose...\n",
       "4  when i was little my parents took me along to ...\n",
       "5   \" it appears that many critics find the idea ...\n",
       "6  the second attempt by a new york intellectual ...\n",
       "7  i don't know who to blame ,  the timid writers...\n",
       "8  this film is mediocre at best .  angie harmon ...\n",
       "9  the film is bad .  there is no other way to sa..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tempfile import mkstemp\n",
    "from shutil import move, copymode\n",
    "from os import fdopen, remove\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def replace(file_path):\n",
    "    #Create temp file\n",
    "    fh, abs_path = mkstemp()\n",
    "    with fdopen(fh,'w') as new_file:\n",
    "        with open(file_path) as old_file:\n",
    "            for line in old_file:\n",
    "                new_file.write(re.sub(\"  \" , \" \", line))\n",
    "\n",
    "    #Copy the file permissions from the old file to the new file\n",
    "    copymode(file_path, abs_path)\n",
    "    #Remove original file\n",
    "    remove(file_path)\n",
    "    #Move new file\n",
    "    move(abs_path, file_path)\n",
    "\n",
    "replace('aclImdb/test-neg.txt')\n",
    "replace('aclImdb/train-pos.txt')\n",
    "replace('aclImdb/test-pos.txt')\n",
    "replace('aclImdb/test-neg.txt')\n",
    "replace('aclImdb/train-unsup.txt')\n",
    "\n",
    "train_neg = pd.read_csv('aclImdb/train-neg.txt', header = None, delimiter = \"\\n\")\n",
    "train_pos = pd.read_csv('aclImdb/train-pos.txt', header = None, delimiter = \"\\n\")\n",
    "train_unsup = pd.read_csv('aclImdb/train-unsup.txt', header = None, delimiter = \"\\n\")\n",
    "test_pos = pd.read_csv('aclImdb/test-pos.txt', header = None, delimiter = \"\\n\")\n",
    "test_neg = pd.read_csv('aclImdb/test-neg.txt', header = None, delimiter = \"\\n\")\n",
    "\n",
    "\n",
    "all_train = pd.concat([train_neg, train_pos, train_unsup], ignore_index=True)\n",
    "\n",
    "all_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "with open('aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "        split = ['train','test','extra','extra'][line_no//25000]  # 25k train, 25k test, 50k unlabled data\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentDocument(words=['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'teachers', '\"', '.', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'teachers', '\"', '.', 'the', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'i', 'immediately', 'recalled', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'at', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'high', '.', 'a', 'classic', 'line', ':', 'inspector', ':', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'student', ':', 'welcome', 'to', 'bromwell', 'high', '.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', '.', 'what', 'a', 'pity', 'that', 'it', \"isn't\", '!'], tags=[0], split='train', sentiment=1.0)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first document \n",
    "doc_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two main categories of doc vector models: PV-DM and PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(cores)\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "# speed setup by sharing results of 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)', <gensim.models.doc2vec.Doc2Vec object at 0x2b183b510>), ('Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)', <gensim.models.doc2vec.Doc2Vec object at 0x2b183b5d0>), ('Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)', <gensim.models.doc2vec.Doc2Vec object at 0x2b183b650>), ('dbow+dmm', <gensim.test.test_doc2vec.ConcatenatedDoc2Vec object at 0x2b1839650>), ('dbow+dmc', <gensim.test.test_doc2vec.ConcatenatedDoc2Vec object at 0x227bda150>)])\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "# Like the author mentioned in the paper, we can concatenate different Doc2Vec models\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])\n",
    "\n",
    "print(models_by_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "# to train a logistic regressor\n",
    "def sk_logistic_regressor(y, X ):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    return lr\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    \n",
    "#     predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "    predictor = sk_logistic_regressor(train_targets, train_regressors)  \n",
    "    # use doccument vector in training set to train logistic regressor\n",
    "\n",
    "    test_data = test_set\n",
    "    \n",
    "    # case 1. for inference, a given sentence is feeded to the model, and a inferred doc vector will be given\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    \n",
    "    # case 2. for evaluation, we use doc vectors in testing set to examine whether our logistic regressor performs well\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_set]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word vectors and doc vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2020-04-18 23:45:15.543186\n",
      "*0.506000 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 0.0s 0.3s\n",
      "*0.512000 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 0.0s 8.9s\n",
      "*0.506000 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 0.0s 0.2s\n",
      "*0.498800 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 0.0s 32.4s\n",
      "*0.506000 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 0.0s 0.3s\n",
      "*0.498800 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 0.0s 4.5s\n",
      "*0.506040 : 1 passes : dbow+dmm 0.0s 0.6s\n",
      "*0.490800 : 1 passes : dbow+dmm_inferred 0.0s 7.1s\n",
      "*0.506040 : 1 passes : dbow+dmc 0.0s 0.6s\n",
      "*0.492800 : 1 passes : dbow+dmc_inferred 0.0s 12.8s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.441840 : 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 25.9s 0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.263840 : 2 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 13.8s 0.5s\n",
      "*0.265000 : 2 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 21.5s 0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.225280 : 2 passes : dbow+dmm 0.0s 1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.263080 : 2 passes : dbow+dmc 0.0s 1.0s\n",
      "completed pass 2 at alpha 0.022600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.337840 : 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 50.4s 0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.130160 : 3 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 28.1s 0.5s\n",
      "*0.191560 : 3 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 41.7s 0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.125520 : 3 passes : dbow+dmm 0.0s 1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.128840 : 3 passes : dbow+dmc 0.0s 0.9s\n",
      "completed pass 3 at alpha 0.020200\n",
      "*0.256840 : 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 69.4s 0.3s\n",
      "*0.112600 : 4 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 40.7s 16.3s\n",
      "*0.168760 : 4 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 65.3s 0.3s\n",
      "*0.111200 : 4 passes : dbow+dmm 0.0s 1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.111720 : 4 passes : dbow+dmc 0.0s 1.1s\n",
      "completed pass 4 at alpha 0.017800\n",
      "*0.209080 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 108.3s 0.5s\n",
      "*0.220400 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 108.3s 8.7s\n",
      "*0.104960 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 61.3s 0.4s\n",
      "*0.113200 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 61.3s 3.5s\n",
      "*0.160840 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 91.7s 0.3s\n",
      "*0.201200 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 91.7s 4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.105360 : 5 passes : dbow+dmm 0.0s 1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.112400 : 5 passes : dbow+dmm_inferred 0.0s 7.7s\n",
      "*0.105080 : 5 passes : dbow+dmc 0.0s 0.8s\n",
      "*0.104400 : 5 passes : dbow+dmc_inferred 0.0s 11.9s\n",
      "completed pass 5 at alpha 0.015400\n",
      "*0.186240 : 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 128.5s 0.3s\n",
      "*0.103080 : 6 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 79.7s 0.4s\n",
      "*0.154200 : 6 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 112.6s 0.3s\n",
      "*0.104320 : 6 passes : dbow+dmm 0.0s 1.0s\n",
      "*0.102760 : 6 passes : dbow+dmc 0.0s 31.7s\n",
      "completed pass 6 at alpha 0.013000\n",
      "*0.177320 : 7 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 157.7s 0.5s\n",
      " 0.103560 : 7 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 92.8s 0.4s\n",
      "*0.151720 : 7 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 138.5s 0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.104080 : 7 passes : dbow+dmm 0.0s 1.0s\n",
      " 0.103880 : 7 passes : dbow+dmc 0.0s 1.1s\n",
      "completed pass 7 at alpha 0.010600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-339-8ab45a6ec6d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0melapsed_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0melapsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%.1f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0melapsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 10)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list,total_examples=100000, epochs=epoch)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.102760 dbow+dmc\n",
      "0.103080 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "0.104080 dbow+dmm\n",
      "0.104400 dbow+dmc_inferred\n",
      "0.112400 dbow+dmm_inferred\n",
      "0.113200 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred\n",
      "0.151720 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
      "0.177320 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
      "0.201200 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred\n",
      "0.207000 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred\n"
     ]
    }
   ],
   "source": [
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "count = 0\n",
    "for model in simple_models:\n",
    "    model.save(\"models/Doc2Vec_{}.model\".format(str(count+1) ))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: \n",
    "#### First, combined Doc2Vec model outperforms single Doc2Vec model. \n",
    "#### Second, different from the conclusion in original paper, PV-DBOW outperforms than PV-DM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment (1): Given a sentence in corpus, let's see whether an inferred sentence is closed to precalculated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 85840...\n",
      "This sentence is : \n",
      " in the middle of nowhere at a bar , the costumers including barkeeper ( clu gulagar ) are enjoying the booze until a mysterious man breaks in and warns them of creatures coming to kill them . they don't believe in him until a bizarre creature kills one of the costumers as each person must try to stay alive and inside the bar as possible to ward off these flesh-eating beasts . exciting and gory-as-hell action-horror comedy is one of the better horror flicks of late . from executive producers wes craven , chris moore , ben affleck and matt damon this is an adrenaline pumping roller-coaster that harks back to the old school style monster movie days . sure the movie may seem plot less even with no explanation of where the monsters come from until the sequels , but it's good quality entertainment for fans of the genre . the film co-stars pulp fiction's dwayne witaker , judah friedlander , henry rollins and balthazar getty ( \" alias \" ) as they give all great performances . i love how the creatures are more traditional style then cgi which doesn't seem to be realistic , i do admire cgi unless it's well done . the gore is non-stop in a cartoony \" dead alive \" or \" re-animator \" kind of way as you see eyeballs ripped out , heads blowing up and so much blood you need an umbrella . i highly recommend this throwback to old school horror movies to any fan , it's great fun . also recommended : \" from dusk till dawn \" , \" chud \" , \" re-animator \" , \" evil dead 1 & 2 \" , \" the toxic avenger \" , \" basket case \" , \" it's alive ( 1974 ) \" , \" the mist \" , \" pumpkinhead \" , \" demons 1 & 2 \" , \" dawn of the dead ( 1978 and 2004 ) \" , \" night of the living dead ( 1968 and 1990 ) \" , \" hatchet \" , \" slither \" , \" poultrygeist : night of the chicken dead \" , \" diary of the dead \" , \" street trash \" , \" day of the dead ( 1985 ) \" , \" the blob ( 1988 ) \" , \" the descent \" , \" the pit \" , \" bad taste \" , \" the thing ( 1982 ) \" , \" grindhouse \" , \" the host \" and \" the hills have eyes ( 1977 and 2006 ) \" .\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "\n",
    "# have a look at what this randomly picked sentence is\n",
    "sentence = \" \".join( doc_list[doc_id].words )\n",
    "print( \"This sentence is : \\n %s\" % (sentence) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc (doc_id=) 85840...\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8):\n",
      " [(85840, 0.8668636083602905), (38419, 0.4256206154823303), (15248, 0.4232979118824005)]\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8):\n",
      " [(85840, 0.9439302682876587), (27649, 0.7646265625953674), (81444, 0.7600319385528564)]\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8):\n",
      " [(28388, 0.9203461408615112), (27931, 0.8856551647186279), (13568, 0.8851773738861084)]\n"
     ]
    }
   ],
   "source": [
    "# from each doc vector model, find the doc vector most similar to the inferred doc vector of given sentence\n",
    "print('for doc (doc_id=) %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment (2): Given a sentence the model has never seen, find the most similar sentence in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of doc(doc_id=28891) is 156, change_ratio = 0.1, number of changed word = 15\n",
      "\n",
      "Before change : \n",
      " i went to see this because i have some friends in the ukraine . but the film moved me beyond what i expected by turning out to be a perfect blend of belly holding laughs ( alex's strange use of english ) situational comedy and heaviness bordering on depressing . i loved the range . it made me want to jump in to an old car and hit the road for the ukraine . alex ( hutz ) plays his guide part perfectly and provides a great counterpoint to elijah woods' poker faced earnestness . the film shows the positive side of humanity when ppl of differing cultures can bond and do the right thing when they feel the sincerity of the situation , even when they went into it with preconceived notions and prejudices , and how this can open up doorways into deeply buried memories . there is a lot in this film .\n",
      "\n",
      "After change : \n",
      " i went to see this because i ironside some friends in the ukraine . but the film moved me beyond what i expected by turning out to be a perfect blend of raping holding laughs ( alex's strange use of english ) walters comedy and heaviness bordering on depressing . i loved the range . it made me want scotland' jeanine in to an old ocean and hit squad' road for the ukraine . alex appolonia hutz ) plays his guide part perfectly and provides a great counterpoint to elijah woods' poker peking earnestness jump-cuts the film shows the positive side of humanity when ppl of differing hatful can bond and do the right thing when they feel the sincerity of the situation , even when they went kagome's it with preconceived notions and prejudices , and how this thrakath open up doorways into deeply buried memories . mujer is a lot in quarry film .\n"
     ]
    }
   ],
   "source": [
    "# We try to build a new sentence by substuting some words in given sentences.\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "\n",
    "word_vocab = simple_models[1].wv.index2word  # the vocabulary containing all words\n",
    "word_split = alldocs[doc_id].words\n",
    "\n",
    "change_ratio = 0.1\n",
    "num_change = int( len(word_split) * change_ratio ) # number of exchange randomly\n",
    "print(\"Length of doc(doc_id={}) is {}, change_ratio = {}, number of changed word = {}\\n\".format\n",
    "      (doc_id, len(word_split), change_ratio, num_change ) )\n",
    "\n",
    "print(\"Before change : \\n %s\" % (\" \".join(word_split)) )\n",
    "\n",
    "for i in range(0, num_change):\n",
    "    # 1. random choose 2 different indices（对应选中两个不同位置的单词）\n",
    "    a = np.random.randint(0, len(word_split) )\n",
    "    b = np.random.randint(0, len(word_vocab) )\n",
    "\n",
    "    # 2. exchange the corresponding words of these 2 indices\n",
    "    word_split[a] = word_vocab[b]\n",
    "\n",
    "print(\"\\nAfter change : \\n %s\" % (\" \".join(word_split)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of doc(doc_id=28891) is 156, change_ratio = 0.1, number of changed word = 15\n",
      "\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) :\n",
      " [(28891, 0.7933717966079712), (63339, 0.4593014121055603), (97994, 0.4356613755226135)]\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) :\n",
      " [(28891, 0.9143658876419067), (66472, 0.5530301332473755), (30821, 0.5515593886375427)]\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) :\n",
      " [(28388, 0.7515352368354797), (34497, 0.7475504279136658), (97459, 0.7347123026847839)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of doc(doc_id={}) is {}, change_ratio = {}, number of changed word = {}\\n\".format\n",
    "      (doc_id, len(word_split), change_ratio, num_change ) )\n",
    "\n",
    "for model in simple_models:\n",
    "    inferred_docvec3 = model.infer_vector( word_split )\n",
    "    print( \"%s :\\n %s\" % (model, model.docvecs.most_similar( [inferred_docvec3], topn=3 )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repaet this replacing experiment for multiple times, get average result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 45]\n",
      "Accuracy:[1.0, 1.0, 0.45]\n"
     ]
    }
   ],
   "source": [
    "exp_num = 100  # numebr of experiment\n",
    "change_ratio = 0.2\n",
    "\n",
    "counts_model = [0, 0, 0]\n",
    "\n",
    "for i in range(exp_num):\n",
    "    doc_id = np.random.randint(simple_models[0].docvecs.count)\n",
    "    \n",
    "    word_vocab = simple_models[1].wv.index2word  # the vocabulary containing all words(list)\n",
    "    word_split = alldocs[doc_id].words\n",
    "\n",
    "    num_change = int( len(word_split) * change_ratio ) # number of exchange randomly\n",
    "\n",
    "    for i in range(0, num_change):\n",
    "        # 1. random choose 2 different indices（对应选中两个不同位置的单词）\n",
    "        a = np.random.randint(0, len(word_split) )\n",
    "        b = np.random.randint(0, len(word_vocab) )\n",
    "\n",
    "        # 2. exchange the corresponding words of these 2 indices\n",
    "        word_split[a] = word_vocab[b]\n",
    "\n",
    "#     print(\"Experiment.no:{}, Length of doc(doc_id={}) is {}, change_ratio = {}, number of changed word = {}\\n\".format\n",
    "#       (i+1, doc_id, len(word_split), change_ratio, num_change ) )\n",
    "    \n",
    "    # do inferring for each model\n",
    "    count = 0\n",
    "    for model in simple_models:\n",
    "        inferred_docvec3 = model.infer_vector( word_split )\n",
    "        similar_results = model.docvecs.most_similar( [inferred_docvec3], topn=3 )\n",
    "        \n",
    "        if doc_id in [ tup[0] for tup in similar_results ]:\n",
    "            counts_model[count] += 1\n",
    "        \n",
    "#         print( \"%s :\\n %s\" % (model,  similar_results ))\n",
    "        \n",
    "        count +=1 \n",
    "#     print(\"\\n\\n\")\n",
    "    \n",
    "print(counts_model)\n",
    "print(\"Accuracy:{}\".format( [ ele/exp_num for ele in counts_model ] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ！！！ to do！！！\n",
    "### Experiment(3): Does Doc2vec models perform well for inferring never-seen-before paragraphs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 15000 test-sentiment, 90000 seen docs, 10000 unseen docs\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs2 = []  # will hold all docs in original order\n",
    "with open('aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        # 0 <= line_no < 100000 \n",
    "        \n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "        \n",
    "        if line_no >= 0 and line_no < 5000:\n",
    "            split = 'unseen_pos'\n",
    "        elif line_no >= 12500 and line_no < 17500:\n",
    "            split = 'unseen_neg'\n",
    "        elif line_no >= 50000:\n",
    "            split = 'extra'  # 25k train, 25k test, 50k unlabled data\n",
    "        elif line_no >= 5000 and line_no < 12500:  # 7500\n",
    "            split = 'test'\n",
    "        elif line_no >= 17500 and line_no < 25000:\n",
    "            split = 'test'\n",
    "        else:\n",
    "            split = 'train'\n",
    "        \n",
    "        # 0～12499：positive; 12500~24999:negetive; 25000~37499:positive; 37500~49999: negative\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        \n",
    "        alldocs2.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs2 = [doc for doc in alldocs2 if doc.split == 'train']\n",
    "test_doc2 = [doc for doc in alldocs2 if doc.split == 'test']\n",
    "# test_docs2 = [doc for doc in alldocs if doc.split == 'test']\n",
    "\n",
    "unseen_pos_docs = [doc for doc in alldocs2 if doc.split == 'unseen_pos']\n",
    "unseen_neg_docs = [doc for doc in alldocs2 if doc.split == 'unseen_neg']\n",
    "\n",
    "doc_seen = [ele for ele in alldocs2 if\n",
    "            (ele.split == 'train' or ele.split == 'test' or ele.split == 'extra')]  # the document the model has ever \"seen\"\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment, %d seen docs, %d unseen docs' % (len(alldocs2), len(train_docs2),\n",
    "                                                                    len(test_doc2), len(doc_seen),\n",
    "                                                                     len(unseen_pos_docs) + len(unseen_neg_docs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "doc_unseen = copy.deepcopy(unseen_pos_docs)\n",
    "doc_unseen.extend( copy.deepcopy(unseen_neg_docs) )\n",
    "shuffle(doc_seen)\n",
    "\n",
    "print(doc_unseen.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2). build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "simple_models2 = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "\n",
    "    # PV-DBOW\n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "simple_models2[0].build_vocab(alldocs2)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "\n",
    "# speed setup by sharing results of 1st model's vocabulary scan\n",
    "for model in simple_models2[1:]:\n",
    "    model.reset_from(simple_models2[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name2 = OrderedDict((str(model), model) for model in simple_models2)  # 把三个单一模型加入进去\n",
    "\n",
    "# # 把两个合成模型加入进去\n",
    "models_by_name2['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models2[1], simple_models2[2]])\n",
    "models_by_name2['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models2[1], simple_models2[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<gensim.models.doc2vec.Doc2Vec at 0x486f67250>,\n",
       " <gensim.models.doc2vec.Doc2Vec at 0x486f67350>,\n",
       " <gensim.models.doc2vec.Doc2Vec at 0x486f674d0>]"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_models2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3).Train Doc2vec model, and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2020-04-19 08:30:09.688344\n",
      "*0.512800 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 0.0s 0.5s\n",
      "*0.506000 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 0.0s 3.1s\n",
      "*0.512800 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 0.0s 0.3s\n",
      "*0.496667 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 0.0s 1.3s\n",
      "*0.512800 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 0.0s 0.3s\n",
      "*0.495333 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 0.0s 1.7s\n",
      "*0.512933 : 1 passes : dbow+dmm 0.0s 0.5s\n",
      "*0.502667 : 1 passes : dbow+dmm_inferred 0.0s 2.4s\n",
      "*0.512933 : 1 passes : dbow+dmc 0.0s 0.4s\n",
      "*0.480667 : 1 passes : dbow+dmc_inferred 0.0s 3.3s\n",
      "completed pass 1 at alpha 0.025000\n",
      "*0.433333 : 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 26.2s 0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.266467 : 2 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 13.0s 0.5s\n",
      "*0.262067 : 2 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 19.8s 0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.224667 : 2 passes : dbow+dmm 0.0s 0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.266333 : 2 passes : dbow+dmc 0.0s 0.9s\n",
      "completed pass 2 at alpha 0.022600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.331533 : 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 42.9s 0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.136000 : 3 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 25.3s 0.5s\n",
      "*0.185400 : 3 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 37.9s 0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.128400 : 3 passes : dbow+dmm 0.0s 1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.134800 : 3 passes : dbow+dmc 0.0s 0.9s\n",
      "completed pass 3 at alpha 0.020200\n",
      "*0.264067 : 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 61.8s 0.4s\n",
      "*0.109600 : 4 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 37.5s 0.4s\n",
      "*0.166467 : 4 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 56.5s 0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.109000 : 4 passes : dbow+dmm 0.0s 1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.110800 : 4 passes : dbow+dmc 0.0s 1.0s\n",
      "completed pass 4 at alpha 0.017800\n",
      "*0.215400 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 81.5s 0.3s\n",
      "*0.257333 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 81.5s 2.2s\n",
      "*0.102867 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 49.4s 0.3s\n",
      "*0.133333 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 49.4s 1.1s\n",
      "*0.154800 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 74.9s 0.3s\n",
      "*0.212000 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 74.9s 1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.103467 : 5 passes : dbow+dmm 0.0s 0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.134000 : 5 passes : dbow+dmm_inferred 0.0s 2.6s\n",
      "*0.105000 : 5 passes : dbow+dmc 0.0s 0.7s\n",
      "*0.138000 : 5 passes : dbow+dmc_inferred 0.0s 3.5s\n",
      "completed pass 5 at alpha 0.015400\n",
      "*0.189067 : 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 101.8s 0.2s\n",
      "*0.100267 : 6 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 61.9s 0.3s\n",
      "*0.149333 : 6 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 93.8s 0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.100733 : 6 passes : dbow+dmm 0.0s 0.8s\n",
      "*0.101333 : 6 passes : dbow+dmc 0.0s 98.5s\n",
      "completed pass 6 at alpha 0.013000\n",
      "*0.178933 : 7 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 143.9s 0.3s\n",
      " 0.103267 : 7 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 87.2s 0.5s\n",
      "*0.146200 : 7 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 115.3s 0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.102467 : 7 passes : dbow+dmm 0.0s 0.9s\n",
      " 0.104000 : 7 passes : dbow+dmc 0.0s 0.7s\n",
      "completed pass 7 at alpha 0.010600\n",
      "*0.174733 : 8 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 143.0s 0.2s\n",
      " 0.102933 : 8 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 86.7s 0.3s\n",
      "*0.143267 : 8 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 130.4s 0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.101800 : 8 passes : dbow+dmm 0.0s 0.8s\n",
      " 0.105267 : 8 passes : dbow+dmc 0.0s 0.7s\n",
      "completed pass 8 at alpha 0.008200\n",
      "*0.170733 : 9 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 162.5s 0.2s\n",
      " 0.105000 : 9 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 99.0s 0.3s\n",
      "*0.142467 : 9 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 152.2s 0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.103067 : 9 passes : dbow+dmm 0.0s 0.8s\n",
      " 0.106000 : 9 passes : dbow+dmc 0.0s 0.7s\n",
      "completed pass 9 at alpha 0.005800\n",
      "*0.168600 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 182.4s 0.2s\n",
      "*0.210000 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 182.4s 2.2s\n",
      " 0.103933 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 110.3s 0.3s\n",
      " 0.138667 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 110.3s 1.1s\n",
      " 0.142600 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 168.1s 0.4s\n",
      " 0.223333 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 168.1s 1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.103467 : 10 passes : dbow+dmm 0.0s 0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.135333 : 10 passes : dbow+dmm_inferred 0.0s 2.7s\n",
      " 0.105267 : 10 passes : dbow+dmc 0.0s 0.7s\n",
      "*0.134667 : 10 passes : dbow+dmc_inferred 0.0s 3.4s\n",
      "completed pass 10 at alpha 0.003400\n",
      "END 2020-04-19 09:12:13.070043\n"
     ]
    }
   ],
   "source": [
    "alpha, min_alpha, passes = (0.025, 0.001, 10)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "model_save_path = \"models2/\"\n",
    "\n",
    "best_error2 = defaultdict(lambda :1.0)\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "\n",
    "    # doc_list就是之前定义的，其每个元素都是一个SentimentDocument\n",
    "    shuffle(doc_seen)  # shuffling gets best results\n",
    "\n",
    "    # models_by_name里面有5个模型（3单一 + 2合成）\n",
    "    for name, train_model in models_by_name2.items():\n",
    "        # train_model是当前所考察的模型\n",
    "\n",
    "        # 1.train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "\n",
    "        # 这是我们之前定义的上下文管理器\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_seen, total_examples=900000, epochs=epoch)  # 每次训练的epoch数是递增的（不懂这样搞有什么意义）\n",
    "            duration = '%.1f' % elapsed()  # 这里调用elapsed()，返回的是现在距离进入该上下文时的时间间隔\n",
    "        # 所有句向量都训练好了（训练了\"epoch\"个epoch）\n",
    "\n",
    "        # 2.evaluate\n",
    "        eval_duration = ''\n",
    "\n",
    "        # 这是我们之前定义的上下文管理器\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs2, test_doc2)\n",
    "\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error2[name]:\n",
    "            best_error2[name] = err\n",
    "            best_indicator = '*'  # 如果该模型这一轮的训练误差比上一轮低，就打个星号\n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "\n",
    "        # 3. infer\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs2, test_doc2, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error2[name + '_inferred']:\n",
    "                best_error2[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "\n",
    "print(\"END %s\" % str(datetime.datetime.now()))\n",
    "\n",
    "# save the model\n",
    "count = 0\n",
    "for model in simple_models2:\n",
    "    model.save(\"models2/Doc2Vec_{}.model\".format(str(count+1) ))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100267 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "0.100733 dbow+dmm\n",
      "0.101333 dbow+dmc\n",
      "0.133333 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred\n",
      "0.134000 dbow+dmm_inferred\n",
      "0.134667 dbow+dmc_inferred\n",
      "0.142467 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
      "0.168600 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
      "0.210000 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred\n",
      "0.212000 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred\n"
     ]
    }
   ],
   "source": [
    "for rate, name in sorted((rate, name) for name, rate in best_error2.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, get inferred vector for unknown sentences, and use these inferred paragraph vectors to do sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8), error_rate:0.1852, error_count:1852, test_count:10000\n",
      "model:Doc2Vec(dbow,d100,n5,mc2,s0.001,t8), error_rate:0.1281, error_count:1281, test_count:10000\n",
      "model:Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8), error_rate:0.2025, error_count:2025, test_count:10000\n"
     ]
    }
   ],
   "source": [
    "### we can tune the hyperparameter: infer_steps\n",
    "for model in simple_models2:\n",
    "    error_rate, error_count, test_count, predicator = error_rate_for_model(model, \n",
    "                                                train_docs2, doc_unseen, infer_steps=3, infer=True, infer_subsample=1) \n",
    "    \n",
    "    print('model:{}, error_rate:{}, error_count:{}, test_count:{}'.format(model, error_rate, error_count, test_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment (4): Word vector and word similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2vec model is based on Word2Vec model. We will evaluate whether the similarity of word vector has relation to the meaning similarity of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"[('countries', 0.7504833340644836),<br>\\n('cities', 0.685369610786438),<br>\\n('towns', 0.6629876494407654),<br>\\n('cultures', 0.6337417364120483),<br>\\n('civilizations', 0.6286609172821045),<br>\\n('armies', 0.6059571504592896),<br>\\n('tribes', 0.5915517807006836),<br>\\n('regimes', 0.5871071815490723),<br>\\n('communities', 0.5733639001846313),<br>\\n('denominations', 0.5725681781768799)]\",\n",
       " \"[('comedy-drama', 0.44841188192367554),<br>\\n('brownstone', 0.41120976209640503),<br>\\n('commentary', 0.4006381034851074),<br>\\n('displaced', 0.394894540309906),<br>\\n('maplins', 0.38663017749786377),<br>\\n('brainy', 0.3835850656032562),<br>\\n('loudspeakers', 0.382948637008667),<br>\\n('advices', 0.3815535604953766),<br>\\n('opiate', 0.37693917751312256),<br>\\n('healer', 0.37368127703666687)]\",\n",
       " '[(\\'countries\\', 0.699311375617981),<br>\\n(\\'governments\\', 0.6510030031204224),<br>\\n(\\'cultures\\', 0.6487419605255127),<br>\\n(\\'organizations\\', 0.6240066289901733),<br>\\n(\\'economies\\', 0.6165152788162231),<br>\\n(\"states\\'\", 0.6150784492492676),<br>\\n(\\'institutions\\', 0.5912648439407349),<br>\\n(\\'societies\\', 0.5903887748718262),<br>\\n(\\'rulers\\', 0.5901978015899658),<br>\\n(\\'populations\\', 0.5853989124298096)]']"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "\n",
    "word_models = simple_models[:]\n",
    "\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "\n",
    "similars_per_model = [str(model.most_similar(word, topn=10)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similars_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'nations' (240 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)</th><th>Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)</th><th>Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)</th></tr><tr><td>[('countries', 0.7504833340644836),<br>\n",
       "('cities', 0.685369610786438),<br>\n",
       "('towns', 0.6629876494407654),<br>\n",
       "('cultures', 0.6337417364120483),<br>\n",
       "('civilizations', 0.6286609172821045),<br>\n",
       "('armies', 0.6059571504592896),<br>\n",
       "('tribes', 0.5915517807006836),<br>\n",
       "('regimes', 0.5871071815490723),<br>\n",
       "('communities', 0.5733639001846313),<br>\n",
       "('denominations', 0.5725681781768799)]</td><td>[('comedy-drama', 0.44841188192367554),<br>\n",
       "('brownstone', 0.41120976209640503),<br>\n",
       "('commentary', 0.4006381034851074),<br>\n",
       "('displaced', 0.394894540309906),<br>\n",
       "('maplins', 0.38663017749786377),<br>\n",
       "('brainy', 0.3835850656032562),<br>\n",
       "('loudspeakers', 0.382948637008667),<br>\n",
       "('advices', 0.3815535604953766),<br>\n",
       "('opiate', 0.37693917751312256),<br>\n",
       "('healer', 0.37368127703666687)]</td><td>[('countries', 0.699311375617981),<br>\n",
       "('governments', 0.6510030031204224),<br>\n",
       "('cultures', 0.6487419605255127),<br>\n",
       "('organizations', 0.6240066289901733),<br>\n",
       "('economies', 0.6165152788162231),<br>\n",
       "(\"states'\", 0.6150784492492676),<br>\n",
       "('institutions', 0.5912648439407349),<br>\n",
       "('societies', 0.5903887748718262),<br>\n",
       "('rulers', 0.5901978015899658),<br>\n",
       "('populations', 0.5853989124298096)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By repeating running the above 2 cells again and again, we can see the experiment results of different selected words.\n",
    "\n",
    "### Conclusion:\n",
    "#### (1) Generally, words with similar meaning have similar word vectors;\n",
    "#### (2) The word vectors in PV-DM models outperforms in PV-DBOW. That's because when training DBOW, word vectors are not trained with paragrah vectors, because requires only using  paragraph vectors to predict words in context.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
